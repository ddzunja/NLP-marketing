{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: BEFORE YOU GET STARTED\n",
    "\n",
    "PLEASE DOWNLOAD MODEL ON [THIS](https://drive.google.com/file/d/1Tx6stkf_pEP7Mis_B4MiAD0yHZMa7jdJ/view?usp=sharing) LINK (~100MB), EXTRACT ALL FILES TO `results` FOLDER\n",
    "\n",
    "WE'VE PUT COMMENTS ON PACKAGES LIKE `hunspell` WHICH RUN ONLY DURING PREPROCESSING! SINCE YOU DOWNLOADED ALL FILES, RUNNING THIS CODE WILL JUST READ THE FILES, THAT ALREADY CONTAIN PROCESSED WORDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Code we used to download dataset.\n",
    "\n",
    "No need to run this since, models are already saved :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Amazon_Instant_Video', \n",
    "# 'Apps_for_Android', \n",
    "# 'Automotive', \n",
    "# 'Baby', \n",
    "# 'Beauty', \n",
    "# 'Digital_Music', \n",
    "# 'Grocery_and_Gourmet_Food', \n",
    "# 'Health_and_Personal_Care', \n",
    "# 'Home_and_Kitchen', \n",
    "# 'Kindle_Store'\n",
    "\n",
    "#DOWLOAD DATASETS\n",
    "\n",
    "## UNCOMMENT to test :)\n",
    "#!python Download_files.py -f amvi apps auto baby beau dgmu food heal hmkt kind  -dt r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts,datapath\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora \n",
    "from gensim.matutils import Sparse2Corpus, Scipy2Corpus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "# from hunspell import HunSpell # Installing this caused a lot of hassle\n",
    "from multiprocessing import pool\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "import pickle\n",
    "import nltk\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from itertools import chain\n",
    "from scipy.sparse import coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA_sk\n",
    "sns.set()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# spellchecker = HunSpell('dicts_hun/en_US.dic',\n",
    "#                         'dicts_hun/en_US.aff')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "names = ['Amazon_Instant_Video', \n",
    "         'Apps_for_Android', \n",
    "         'Automotive', \n",
    "         'Baby', \n",
    "         'Beauty', \n",
    "         'Digital_Music', \n",
    "         'Grocery_and_Gourmet_Food', \n",
    "         'Health_and_Personal_Care', \n",
    "         'Home_and_Kitchen', \n",
    "         'Kindle_Store'\n",
    "        ]\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word preprocessing! It doesn't take a lot of time, if files in `results` folder exists, preprocessing won't be run again and files will be loaded!\n",
    "\n",
    "1. Datasets are loaded (this is **SLOW**, but parallized! You can set number of cores with `paralelize_reading`. TAKES A LOT OF **RAM**)\n",
    "2. `n` comments are randomly chosen\n",
    "3. Remove punctionation and digits\n",
    "4. Remove STOP words 'he', 'she', 'is' etc.\n",
    "5. Remove wrongly spelled words (we thought of spelling them correctly, but it slows process 50x times!)\n",
    "6. Stemming! `stemmer.stem('ponies') -> 'poni`\n",
    "7. Remove rare words\n",
    "8. Create dictionary, vocabulary, corpus and SAVE THEM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rerun_preprocessing = 0 ## If you want run code again even if files exist\n",
    "paralelize_reading = 12 ## Number of cores used for reading, set to 1 if you don't want to paralelize\n",
    "\n",
    "def read_dataset(name):\n",
    "    df = getDF('data/reviews/reviews_{}.json.gz'.format(name))\n",
    "    print('Reading {} finished!'.format(name))\n",
    "    return df.reviewText + ' ' + df.summary + ' '\n",
    "    \n",
    "\n",
    "if ((os.path.isfile('results/corpus.pickle') and \n",
    "    os.path.isfile('results/dictionary.pickle') and\n",
    "    os.path.isfile('results/vocab.pickle')) and not(force_rerun_preprocessing)):\n",
    "    \n",
    "    with open('results/corpus.pickle', 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    with open('results/dictionary.pickle', 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "    with open('results/vocab.pickle', 'rb') as f:\n",
    "        vocabulary = pickle.load(f)\n",
    "        \n",
    "    print('Files already exist! No need to load this again!')\n",
    "else:\n",
    "\n",
    "    ans = ''\n",
    "\n",
    "#     for name in tqdm_notebook(names):\n",
    "\n",
    "#         df = getDF('data/reviews/reviews_{}.json.gz'.format(name))\n",
    "#         ans += df.reviewText + ' ' + df.summary + ' '\n",
    "\n",
    "    ## Let's use those cores :p\n",
    "    print('Reading datasets')\n",
    "    array_ans = Parallel(n_jobs=paralelize_reading)(delayed(read_dataset)(name) for name in names)\n",
    "    \n",
    "    for temp in array_ans:\n",
    "        ans+=temp\n",
    "\n",
    "    n = 50000\n",
    "\n",
    "    re_punctuation = re.compile('['+string.punctuation+']')\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    stop = stopwords.words('english')\n",
    "    preprocessed_comments = []\n",
    "    for comment in tqdm_notebook(np.random.choice(ans, n)):\n",
    "        comment = re_punctuation.sub(' ', comment)\n",
    "        comment = tokenizer.tokenize(comment)\n",
    "        comment = [x for x in comment if not any(c.isdigit() for c in x)]\n",
    "        comment = [word for word in comment if word not in stop]\n",
    "        comment = [stemmer.stem(x) for x in comment if spellchecker.spell(x)]\n",
    "        comment = [x for x in comment if len(x) > 3]\n",
    "        preprocessed_comments.append(comment)\n",
    "    \n",
    "    \n",
    "    wordFrequency = Counter()\n",
    "    for comment in tqdm_notebook(preprocessed_comments):\n",
    "        wordFrequency.update(comment)                                  # Count overall word frequency\n",
    "    print('Unique Words In Comments: {}'.format(len(wordFrequency)))\n",
    "\n",
    "    minimumWordOccurrences = 5\n",
    "    # Remove rare words\n",
    "    print('Removing rare words... ')\n",
    "    texts = [[word for word in comment if wordFrequency[word] > minimumWordOccurrences] for comment in tqdm_notebook(preprocessed_comments)]\n",
    "\n",
    "    print('Creating vocabulary...')\n",
    "    dictionary = corpora.Dictionary(texts)                             # Create word dictionary\n",
    "    vocabulary = [dictionary[i] for i in tqdm_notebook(dictionary.keys())]\n",
    "    print('Documents/Comments: {}'.format(len(texts)))\n",
    "\n",
    "    print('Creating corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tqdm_notebook(preprocessed_comments)] # Create corpus\n",
    "\n",
    "    \n",
    "    print('Saving files...')\n",
    "    with open('results/corpus.pickle', 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    with open('results/dictionary.pickle', 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    with open('results/vocab.pickle', 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "    \n",
    "    print('WE ARE GOOD TO GO!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENSIM LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training is computationaly heavy and can take up to 5 hours, this has been processed on AWS EC2 cluster.\n",
    "Note: If models exist in `results` folder, training won't run again and already existing model will be loaded.\n",
    "You can freely run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rerun_lda = 0\n",
    "\n",
    "if (os.path.isfile('results/model_gensim/model_gensim.model') & (not force_rerun_lda)):\n",
    "    model_gensim = LdaMulticore.load(\"results/model_gensim/model_gensim.model\", mmap='r')\n",
    "    with open('results/model_gensim/perp_gensim.pickle', 'rb') as f:\n",
    "        perp_gensim = pickle.load(f)\n",
    "    with open('results/model_gensim/time_gensim.pickle', 'rb') as f:\n",
    "        time_gensim = pickle.load(f)\n",
    "else:\n",
    "    numberTopics = 50  #Number of topics\n",
    "    model_gensim = LdaMulticore(num_topics=numberTopics,\n",
    "                            id2word=dictionary,\n",
    "                            iterations=10,\n",
    "                            passes=1,\n",
    "                            chunksize=50,\n",
    "                            eta='auto',\n",
    "                            workers=12)\n",
    "\n",
    "    \n",
    "    perp_gensim = []\n",
    "    times_gensim = []\n",
    "    i=0\n",
    "    max_it = 5\n",
    "    min_prep = np.inf\n",
    "    start = time()\n",
    "    for _ in tqdm_notebook(range(100)):\n",
    "        model_gensim.update(corpus)\n",
    "        tmp = np.exp(-1 * model_gensim.log_perplexity(corpus))\n",
    "        perp_gensim.append(tmp)\n",
    "        times_gensim.append(time() - start)\n",
    "        if(tmp<min_prep):\n",
    "            min_prep = tmp;\n",
    "            i = 0\n",
    "        else:\n",
    "            i = i + 1;\n",
    "            if (i==max_it):\n",
    "                break                # if prep increase for max_it number it will break the update procedure \n",
    "    model_gensim.save('results/model_gensim/model_gensim.model')\n",
    "    with open('results/model_gensim/perp_gensim.pickle', 'wb') as f:\n",
    "        pickle.dump(perp_gensim, f)\n",
    "    with open('results/model_gensim/time_gensim.pickle', 'wb') as f:\n",
    "        pickle.dump(times_gensim, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(perp_gensim,'-o', label='Perplexity')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATED TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(model_gensim.get_topics().argsort(axis=1)[:, -10:][:, ::-1], 1):\n",
    "    print('Topic {}: {}'.format(i, ' '.join([vocabulary[id] for id in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLEARN LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING CORPUS SPARSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rerun_sk = 0\n",
    "\n",
    "\n",
    "if os.path.isfile('results/model_sk/sk_lda.pickle') & \\\n",
    "    os.path.isfile('results/model_sk/perplexity_sklearn.pickle') & \\\n",
    "    os.path.isfile('results/model_sk/timestamps_sklearn.pickle') & (not force_rerun_sk):\n",
    "    with open('results/sk_lda.pickle', 'rb') as f:\n",
    "        sk_lda = pickle.load(f)\n",
    "    with open('results/model_sk/timestamps_sklearn.pickle') as f:\n",
    "        timestamps_sklearn = pickle.load(f)\n",
    "    with open('results/model_sk/perplexity_sklearn.pickle') as f:\n",
    "        perplexity_sklearn = pickle.load(f)\n",
    "        \n",
    "    print('File loaded!')\n",
    "else: \n",
    "\n",
    "    arr_1 = np.asarray([np.array(list(map(np.array, corpus[i]))) for i in range(len(corpus))])\n",
    "    data = np.array([])\n",
    "    row = np.array([])\n",
    "    column = np.array([])\n",
    "\n",
    "    for i in tqdm_notebook(range(arr_1.shape[0])):\n",
    "        data = np.append(data, arr_1[i][:, 1])\n",
    "        row = np.append(row, i*np.ones(shape=(arr_1[i].shape[0],)))\n",
    "        column = np.append(column, arr_1[i][:,0])\n",
    "        if not(data.shape == row.shape == column.shape):\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    sk_corpus = coo_matrix((data, (row.astype(int), column.astype(int)))).tocsc()\n",
    "\n",
    "    sk_lda = LDA_sk(n_components=50, \n",
    "                    learning_method='online',\n",
    "                    n_jobs=-1,\n",
    "                    max_iter = 1,\n",
    "                    total_samples = 10000)\n",
    "\n",
    "    perplexity_sklearn = []\n",
    "    timestamps_sklearn = []\n",
    "    start = time()\n",
    "    for _ in tqdm_notebook(range(100)):\n",
    "        model_sklearn.partial_fit(X)\n",
    "        perp_sklearn.append(model_sklearn.perplexity(X))        # Append metric\n",
    "        times_sklearn.append(time()-start)\n",
    "\n",
    "    print('Saving files...')\n",
    "    with open('results/sk_lda.pickle', 'wb') as f:\n",
    "        pickle.dump(sk_lda, f)\n",
    "    with open('results/model_sk/timestamps_sklearn.pickle') as f:\n",
    "        pickle.dump(timestamps_sklearn, f)\n",
    "    with open('results/model_sk/perplexity_sklearn.pickle') as f:\n",
    "        pickle.dump(perplexity_sklearn, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_top_words=15\n",
    "n_components = 50\n",
    "data = np.array([([vocabulary[i] for i in topic.argsort()[:-n_top_words - 1:-1]]) for topic in sk_lda.components_])\n",
    "topics = pd.DataFrame(data, columns=['word_{}'.format(i) for i in range(1, n_top_words+1)], index=['topic_{}'.format(i) for i in range(1, 1+n_components)])\n",
    "topics.head(15)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA VISUALIZATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "with open('Downloads/results.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "emb = np.array([embeddings[key] for key in embeddings.keys()])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(emb)\n",
    "comp = pca.components_\n",
    "with open('PCA_Amazon.pickle', 'wb') as f:\n",
    "    pickle.dump(comp, f)\n",
    "\n",
    "with open('../../PCA_Amazon.pickle', 'rb') as f:\n",
    "    comp = pickle.load(f)\n",
    "with open('../../Downloads/results.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_topics = [0, 2, 14, 19]\n",
    "n_top_words = 12\n",
    "\n",
    "topics_pca=[]\n",
    "for i, topic in enumerate(lda.get_topics().argsort(axis=1)[:, -n_top_words-1:][:, ::-1]):\n",
    "    if i in n_top_topics:\n",
    "        topics_pca.append([vocabulary[id] for id in topic if vocabulary[id] in embeddings.keys()])\n",
    "        print(topics_pca[-1])\n",
    "topics_pca = np.array(topics_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_components = []\n",
    "for i, topic in enumerate(lda.get_topics().argsort(axis=1)[:, -n_top_words-1:][:, ::-1]):\n",
    "    if i in n_top_topics:\n",
    "        topics_components.append(np.array([[np.array(embeddings[vocabulary[id]]).dot(component) for component in comp] for id in topic if vocabulary[id] in embeddings.keys()]))\n",
    "        print(topics_components[-1])\n",
    "topics_components = np.array(topics_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, topic_components in enumerate(topics_components):\n",
    "    plt.scatter(topic_components[:, 0], topic_components[:, 1])\n",
    "    for txt, point in zip(topics_pca[i], topic_components):\n",
    "        plt.annotate(txt, point)\n",
    "plt.xlabel('principal component 1')\n",
    "plt.ylabel('principal component 2')\n",
    "plt.savefig('pca_topic.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
